---
title:  "Peering Beyond the Gradient Veil with Distributed Auto Differentiation"
image: "images/post/Tranking Brain.png"
author: "Sergey Plis"
date: 2014-03-01T05:00:00Z
description: "This is meta description"
categories: ["Publications"]
tags: ["Paper", "Publications"]
featured: true

---
Title: Peering Beyond the Gradient Veil with Distributed Auto Differentiation
  
Authors: Bradley T Baker, Aashis Khanal, Vince D Calhoun, Barak Pearlmutter, Sergey M Plis
  
Year: 2021
  
Journal: arXiv e-prints
  
Pages: arXiv: 2102.09631
  
Description:
  
Although distributed machine learning has opened up many new and exciting research frontiers, fragmentation of models and data across different machines, nodes, and sites still results in considerable communication overhead, impeding reliable training in real-world contexts. The focus on gradients as the primary shared statistic during training has spawned a number of intuitive algorithms for distributed deep learning; however, gradient-centric training of large deep neural networks (DNNs) tends to be communication-heavy, often requiring additional adaptations such as sparsity constraints, compression, quantization, and more, to curtail bandwidth. We introduce an innovative, communication-friendly approach for training distributed DNNs, which capitalizes on the outer-product structure of the gradient as revealed by the mechanics of auto-differentiation. The exposed structure of the gradient evokes a new class â€¦

  
[View article](https://ui.adsabs.harvard.edu/abs/2021arXiv210209631B/abstract)  
